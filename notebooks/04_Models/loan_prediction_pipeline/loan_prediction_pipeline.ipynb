{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cFFaefbFjT8"
      },
      "source": [
        "# Loan Amount Prediction & Classification Model  \n",
        "### End-to-End Modeling with Preprocessed Lending Data\n",
        "\n",
        "This notebook builds two machine learning models using a fully cleaned and preprocessed dataset (`df_model_cleaned.csv`):\n",
        "\n",
        "1. **A Regression Model**  \n",
        "   Predicts the exact loan amount requested by a borrower.  \n",
        "2. **A Classification Model**  \n",
        "   Categorizes each loan request into Low, Medium, or High amount buckets.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Œ Background\n",
        "\n",
        "The dataset used in this notebook (`df_model_cleaned.csv`) was created in a separate preprocessing workflow by merging and transforming the following cleaned tables:\n",
        "\n",
        "- `borrower_info_cleaned`  \n",
        "- `credit_history_cleaned`  \n",
        "- `delinquency_risk_cleaned`  \n",
        "- `loan_base_cleaned`\n",
        "\n",
        "All cleaning, merging, and feature preparation operations were performed outside this notebook.  \n",
        "Here, we focus solely on modeling, evaluation, and interpretation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“˜ Modeling Workflow Overview\n",
        "\n",
        "### **1. Regression Task**\n",
        "- Select stable, high-signal features  \n",
        "- Train a Random Forest Regressor  \n",
        "- Apply log-transform to stabilize the target variable  \n",
        "- Evaluate performance using:\n",
        "  - MAE  \n",
        "  - RMSE  \n",
        "  - RÂ²  \n",
        "- Compare the model against a baseline predictor  \n",
        "- Extract and visualize feature importances  \n",
        "\n",
        "### **2. Classification Task**\n",
        "- Convert the continuous loan amount into 3 buckets:\n",
        "  - Low (â‰¤10k), Medium (10kâ€“20k), High (>20k)\n",
        "- Build a classification dataset using proven regression features  \n",
        "- Train a Random Forest Classifier  \n",
        "- Evaluate with:\n",
        "  - Accuracy  \n",
        "  - Full classification report (precision, recall, F1-score)  \n",
        "  - Confusion matrix  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Objective\n",
        "\n",
        "The purpose of this notebook is to create a robust and interpretable pipeline for predicting loan amounts and categorizing borrowers into meaningful risk/size segments.  \n",
        "The workflow demonstrates:\n",
        "\n",
        "- Proper handling of mixed data types  \n",
        "- Preprocessing pipelines  \n",
        "- Model training with scikit-learn  \n",
        "- Evaluation against baselines  \n",
        "- Feature importance analysis  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Let's Begin\n",
        "\n",
        "We start by importing all required libraries and loading the combined modeling dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFmueUI4GPh4"
      },
      "source": [
        "## 1. Regression Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLXXxCzhOyjs"
      },
      "source": [
        "### Import Required Libraries\n",
        "\n",
        "In this section, we import all necessary Python libraries for data manipulation, visualization, preprocessing, model training, and evaluation.  \n",
        "These include:\n",
        "- **Pandas & NumPy** for data handling  \n",
        "- **Matplotlib & Seaborn** for exploratory visualizations  \n",
        "- **Scikit-learn** modules for preprocessing, encoding, modeling, and performance metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssag47r4OxdJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a7txt_mO20Z"
      },
      "source": [
        "### Load Preprocessed Modeling Dataset\n",
        "\n",
        "We load the `df_model_cleaned.csv` file, which serves as the final input dataset for model development.  \n",
        "This dataset was previously created in a separate preprocessing notebook by merging and transforming the following cleaned tables:\n",
        "\n",
        "- `borrower_info_cleaned`\n",
        "- `credit_history_cleaned`\n",
        "- `delinquency_risk_cleaned`\n",
        "- `loan_base_cleaned`\n",
        "\n",
        "Since those preparation steps were completed earlier, only the resulting dataset is loaded here.  \n",
        "Below, we display the dataset's shape and the first few rows to verify its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x34H7ngOCwa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"df_model_cleaned.csv\")\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI_jEKfKPOfU"
      },
      "source": [
        "### Select Relevant Features for Modeling\n",
        "\n",
        "In this step, we define the feature set to be used for training the model.  \n",
        "We exclude noisy or unstable variables (such as delinquency-related columns) and keep only the most reliable and predictive features.  \n",
        "The selected feature list includes both numerical and categorical variables:\n",
        "\n",
        "- Income and credit limit indicators  \n",
        "- Account and credit activity metrics  \n",
        "- Employment information  \n",
        "- Categorical attributes such as loan purpose, home ownership, and application type  \n",
        "\n",
        "After defining the feature set, we construct a clean regression dataset by selecting the chosen columns along with the target variable `loan_amnt`, and removing any rows containing missing values.  \n",
        "Finally, we create the feature matrix **X** and target vector **y** for subsequent modeling steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGtoNSLWPP3E"
      },
      "outputs": [],
      "source": [
        "# Remove noisy columns and keep only the strongest predictive features\n",
        "feature_cols = [\n",
        "    'annual_inc_capped',\n",
        "    'term_clean',\n",
        "    'total_bc_limit_capped',\n",
        "    'total_il_high_credit_limit_capped',\n",
        "    'total_acc',\n",
        "    'num_rev_accts',\n",
        "    'emp_length_clean',\n",
        "    'purpose',          # Categorical\n",
        "    'home_ownership',   # Categorical\n",
        "    'application_type'  # Categorical\n",
        "]\n",
        "\n",
        "# We no longer include noisy variables such as delinquency-related columns\n",
        "print(\"Selected Feature Columns:\")\n",
        "print(feature_cols)\n",
        "\n",
        "# Redefine X and y\n",
        "df_reg = df[feature_cols + ['loan_amnt']].dropna()\n",
        "X = df_reg[feature_cols]\n",
        "y = df_reg['loan_amnt']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nOb_-XjPXuR"
      },
      "source": [
        "### Prepare Regression Dataset\n",
        "\n",
        "We subset the dataframe to include only the selected feature columns along with the target variable `loan_amnt`, and then remove any rows containing missing values.  \n",
        "This results in a clean and consistent dataset for modeling.  \n",
        "\n",
        "Afterward, we define:\n",
        "- **X** â†’ the feature matrix  \n",
        "- **y** â†’ the target variable representing the loan amount  \n",
        "\n",
        "We also print the shape of the regression dataset to verify the final size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi02LzARPaky"
      },
      "outputs": [],
      "source": [
        "df_reg = df[feature_cols + ['loan_amnt']].dropna()\n",
        "\n",
        "print(\"Regression Dataset Shape:\", df_reg.shape)\n",
        "\n",
        "X = df_reg[feature_cols]\n",
        "y = df_reg['loan_amnt']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3fp_9BTPkSe"
      },
      "source": [
        "### Separate Numerical and Categorical Features\n",
        "\n",
        "To prepare the dataset for preprocessing, we identify which features are numerical and which are categorical.  \n",
        "- Numerical columns are detected automatically based on their data types.  \n",
        "- Categorical columns are determined by selecting the remaining features that are not numeric.\n",
        "\n",
        "This separation is required because different preprocessing steps (imputation, encoding, scaling, etc.) will be applied to each feature type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5je1BFvqPkvY"
      },
      "outputs": [],
      "source": [
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "categorical_features = [c for c in feature_cols if c not in numeric_features]\n",
        "\n",
        "print(\"Numeric:\", numeric_features)\n",
        "print(\"Categorical:\", categorical_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Q0hWbYPuNc"
      },
      "source": [
        "### Build Preprocessing Pipelines\n",
        "\n",
        "We define preprocessing steps for both numerical and categorical features:\n",
        "\n",
        "- **Numerical Features:**  \n",
        "  Missing values are imputed using the median, which is robust to outliers.\n",
        "\n",
        "- **Categorical Features:**  \n",
        "  A pipeline is created that first imputes missing values with the most frequent category,  \n",
        "  and then applies One-Hot Encoding while ignoring unseen categories during inference.\n",
        "\n",
        "These preprocessing components are combined into a `ColumnTransformer`,  \n",
        "ensuring the correct transformations are applied to each feature group before model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfln081sPwKZ"
      },
      "outputs": [],
      "source": [
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmyiaZg6PzM-"
      },
      "source": [
        "### Split the Data and Apply Log Transformation to the Target Variable\n",
        "\n",
        "We begin by splitting the dataset into training and testing sets using an 80/20 ratio.  \n",
        "To improve model performance and stabilize variance, we apply a **logarithmic transformation** (`log1p`) to the target variable (`loan_amnt`).  \n",
        "\n",
        "This transformation helps:\n",
        "- Normalize the distribution of loan amounts  \n",
        "- Reduce skewness  \n",
        "- Improve the model's ability to capture patterns in the data  \n",
        "\n",
        "We then print the shapes of the resulting datasets and preview both the original and transformed target values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2FgcOgfP2Q2"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# IMPORTANT: Apply log transformation to the target variable\n",
        "# This normalizes the distribution and reduces model bias.\n",
        "y_train_log = np.log1p(y_train)\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
        "print(\"y_train (original) head:\", y_train.head(3).values)\n",
        "print(\"y_train (log-transformed) head:\", y_train_log.head(3).values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_sALw_eP4VZ"
      },
      "source": [
        "### Build and Train the Random Forest Regression Pipeline\n",
        "\n",
        "We create a full modeling pipeline that includes both preprocessing and model training.  \n",
        "The pipeline consists of:\n",
        "\n",
        "- **Preprocessing Step:**  \n",
        "  Applies numerical imputation, categorical imputation, and one-hot encoding using the previously defined `preprocessor`.\n",
        "\n",
        "- **Modeling Step (Random Forest Regressor):**  \n",
        "  A tree-based ensemble model configured with:\n",
        "  - 200 estimators  \n",
        "  - Maximum depth of 8 to reduce overfitting  \n",
        "  - `min_samples_leaf=100` to enforce more general decision splits  \n",
        "  - Parallel processing enabled via `n_jobs=-1`  \n",
        "\n",
        "The model is trained using the **log-transformed target variable** (`y_train_log`) to improve stability and prediction accuracy.  \n",
        "Once training is completed, we confirm that the model has been successfully fitted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GXIUeE2P8SA"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build the full modeling pipeline\n",
        "rf_reg = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,          # Reduced from 10 to 8 for less overfitting\n",
        "        min_samples_leaf=100, # Increased from 50 to 100 for more general splits\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train the model using the log-transformed target variable\n",
        "rf_reg.fit(X_train, y_train_log)\n",
        "print(\"Model trained using the log-transformed target variable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1OLONt2P-Tb"
      },
      "source": [
        "### Generate Predictions and Evaluate Model Performance\n",
        "\n",
        "After training the model on the log-transformed target, we generate predictions for both the training and test sets.  \n",
        "Since the model outputs values in logarithmic scale, we apply the inverse transformation (`expm1`) to bring predictions back to their original currency scale.\n",
        "\n",
        "We then evaluate model performance using the following metrics calculated on the **actual loan amount values**:\n",
        "\n",
        "- **MAE (Mean Absolute Error):** Measures average absolute prediction error  \n",
        "- **RMSE (Root Mean Squared Error):** Punishes larger errors more heavily  \n",
        "- **RÂ² Score:** Indicates how much variance in the target is explained by the model  \n",
        "\n",
        "Finally, we print the training and testing results to assess model accuracy and generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdAApxCiQD85"
      },
      "outputs": [],
      "source": [
        "# 1. Generate predictions (log-scale outputs)\n",
        "y_pred_log_train = rf_reg.predict(X_train)\n",
        "y_pred_log_test = rf_reg.predict(X_test)\n",
        "\n",
        "# 2. Convert predictions back from log scale (inverse transform)\n",
        "y_pred_train = np.expm1(y_pred_log_train)\n",
        "y_pred_test = np.expm1(y_pred_log_test)\n",
        "\n",
        "# 3. Calculate performance metrics using actual values\n",
        "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(\"=== UPDATED RandomForest Results ===\")\n",
        "print(f\"Train MAE : {mae_train:.2f}\")\n",
        "print(f\"Test MAE  : {mae_test:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Train RMSE: {rmse_train:.2f}\")\n",
        "print(f\"Test RMSE : {rmse_test:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Train R2  : {r2_train:.3f}\")\n",
        "print(f\"Test R2   : {r2_test:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA2WE-GDDfjL"
      },
      "source": [
        "### Establish a Baseline Model for Comparison\n",
        "\n",
        "To evaluate whether our machine learning model provides meaningful improvements, we create a simple baseline model.  \n",
        "This baseline predicts the **mean loan amount from the training set** for every sample in the test set.\n",
        "\n",
        "We then compute MAE, RMSE, and RÂ² for this baseline.  \n",
        "Comparing these metrics with the Random Forest model helps determine how much predictive value the trained model adds beyond a trivial guess.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oYsIkYpV_Vc"
      },
      "outputs": [],
      "source": [
        "# Baseline model: predict the mean loan amount from the training set for all test samples\n",
        "baseline_pred = np.full_like(y_test, y_train.mean(), dtype=float)\n",
        "\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
        "baseline_r2 = r2_score(y_test, baseline_pred)\n",
        "\n",
        "print(\"=== Baseline (mean prediction) ===\")\n",
        "print(f\"Baseline MAE : {baseline_mae:.2f}\")\n",
        "print(f\"Baseline RMSE: {baseline_rmse:.2f}\")\n",
        "print(f\"Baseline R2  : {baseline_r2:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV38sVZ6QINF"
      },
      "source": [
        "### Extract and Display Feature Importances\n",
        "\n",
        "To understand which variables contribute most to the model's predictions,  \n",
        "we extract feature importance scores from the trained Random Forest model.\n",
        "\n",
        "Because categorical features were one-hot encoded during preprocessing,  \n",
        "we first retrieve the full expanded feature name list using `get_feature_names_out()`.  \n",
        "We then pair these names with their corresponding importance values and sort them in descending order.\n",
        "\n",
        "Finally, we display the top 20 most influential features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdeta4-mQIt4"
      },
      "outputs": [],
      "source": [
        "# Extract the expanded feature names after preprocessing\n",
        "feature_names = rf_reg.named_steps['preprocess'].get_feature_names_out()\n",
        "\n",
        "# Retrieve feature importance scores from the trained Random Forest model\n",
        "importances = rf_reg.named_steps['model'].feature_importances_\n",
        "\n",
        "# Create a DataFrame for easier inspection and sort by importance\n",
        "fi = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "fi.head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3xj4C4CECYz"
      },
      "source": [
        "### Visualize the Top Feature Importances\n",
        "\n",
        "To better interpret the most influential predictors in the model,  \n",
        "we visualize the top 10 features based on their importance scores.\n",
        "\n",
        "A horizontal bar plot is generated to highlight which features the Random Forest model relies on most when estimating loan amounts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41PKTxj7QKlr"
      },
      "outputs": [],
      "source": [
        "top_n = 10\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=fi.head(top_n), x='importance', y='feature')\n",
        "plt.title(\"Feature Importance (Top 10)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqtzkuGSEKAz"
      },
      "source": [
        "### Summary Statistics of the Target Variable\n",
        "\n",
        "Before interpreting prediction performance, it is useful to examine the distribution of the target variable (`loan_amnt`).  \n",
        "The summary statistics below provide insights into the central tendency, spread, and overall scale of loan amounts in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvzY3KNUTvLu"
      },
      "outputs": [],
      "source": [
        "df_reg['loan_amnt'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARZ3g0F0EWoW"
      },
      "source": [
        "### Recalculate Baseline Metrics for Comparison\n",
        "\n",
        "Once again, we compute baseline performance metrics using a simple model that predicts the **mean loan amount from the training set** for every test instance.  \n",
        "This provides a straightforward benchmark to evaluate how much the machine learning model improves over a naive prediction strategy.\n",
        "\n",
        "The baseline metrics reported include:\n",
        "- **MAE:** Mean Absolute Error  \n",
        "- **RMSE:** Root Mean Squared Error  \n",
        "- **RÂ²:** Coefficient of Determination  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxR_aGjUUSgB"
      },
      "outputs": [],
      "source": [
        "# Baseline model: predict the mean loan amount from the training set for all test samples\n",
        "baseline_pred = np.full_like(y_test, y_train.mean(), dtype=float)\n",
        "\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
        "baseline_r2 = r2_score(y_test, baseline_pred)\n",
        "\n",
        "print(\"Baseline MAE :\", baseline_mae)\n",
        "print(\"Baseline RMSE:\", baseline_rmse)\n",
        "print(\"Baseline R2  :\", baseline_r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXZt4SUGxYI"
      },
      "source": [
        "## 2. Classification Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeZCSE5MEj2v"
      },
      "source": [
        "### Convert the Target Variable into Loan Amount Buckets (Classification Setup)\n",
        "\n",
        "To transition from regression to a classification problem, we transform the continuous loan amount (`loan_amnt`) into categorical buckets:\n",
        "\n",
        "- **0:** Low (â‰¤ 10,000)  \n",
        "- **1:** Medium (10,001â€“20,000)  \n",
        "- **2:** High (> 20,000)\n",
        "\n",
        "A custom function (`create_loan_bucket`) assigns each loan to its corresponding class, creating the new target variable `loan_bucket`.\n",
        "\n",
        "Next, we define the feature set using predictors that performed well in the regression model.  \n",
        "Finally, we prepare **X** (features) and **y** (class labels) and print their shapes to confirm the dataset is ready for classification modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xsEoI2z51V5"
      },
      "outputs": [],
      "source": [
        "# 1. Convert the continuous target variable into categorical buckets (binning)\n",
        "def create_loan_bucket(amount):\n",
        "    if amount <= 10000:\n",
        "        return 0  # Low\n",
        "    elif amount <= 20000:\n",
        "        return 1  # Medium\n",
        "    else:\n",
        "        return 2  # High\n",
        "\n",
        "# Create the new classification target column\n",
        "df['loan_bucket'] = df['loan_amnt'].apply(create_loan_bucket)\n",
        "\n",
        "print(\"Class Distribution:\")\n",
        "print(df['loan_bucket'].value_counts())\n",
        "\n",
        "# 2. Select features (based on successful regression predictors)\n",
        "feature_cols = [\n",
        "    'annual_inc_capped',\n",
        "    'term_clean',\n",
        "    'total_bc_limit_capped',\n",
        "    'total_il_high_credit_limit_capped',\n",
        "    'total_acc',\n",
        "    'purpose',          # Categorical\n",
        "    'home_ownership'    # Categorical\n",
        "]\n",
        "\n",
        "# 3. Prepare X and y (y is now loan_bucket instead of loan_amnt)\n",
        "X = df[feature_cols]\n",
        "y = df['loan_bucket']\n",
        "\n",
        "print(\"Dataset is ready for classification!\")\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_GOSkGvE4kb"
      },
      "source": [
        "### Build and Train the Classification Pipeline\n",
        "\n",
        "In this section, we prepare the dataset for a multi-class classification task by defining preprocessing steps and training a **Random Forest Classifier**.\n",
        "\n",
        "**1. Feature Type Separation**  \n",
        "We specify which features are numerical and which are categorical so that appropriate preprocessing can be applied.\n",
        "\n",
        "**2. Preprocessing Pipelines**  \n",
        "- Numerical features: missing values are imputed using the median.  \n",
        "- Categorical features: missing values are filled with the most frequent category, followed by One-Hot Encoding to convert text labels into numeric vectors.\n",
        "\n",
        "These transformations are combined into a `ColumnTransformer` to ensure the correct preprocessing is applied automatically.\n",
        "\n",
        "**3. Train/Test Split**  \n",
        "The dataset is split into an 80% training set and a 20% test set.\n",
        "\n",
        "**4. Model Definition**  \n",
        "A `RandomForestClassifier` is used, leveraging:\n",
        "- 100 decision trees  \n",
        "- Full parallelization (`n_jobs=-1`)  \n",
        "- Fixed randomness for reproducibility  \n",
        "\n",
        "The model is wrapped inside a pipeline to ensure preprocessing and prediction flow seamlessly.\n",
        "\n",
        "**5. Model Training and Evaluation**  \n",
        "We train the model, generate predictions on the test set, and evaluate performance using:\n",
        "- **Accuracy score**\n",
        "- **Classification report** (precision, recall, F1-score)\n",
        "- **Confusion matrix**\n",
        "\n",
        "These outputs allow us to assess how well the model classifies loans into Low, Medium, and High buckets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voYQTdGx6Mmp"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Define numerical and categorical feature groups\n",
        "numeric_features = [\n",
        "    'annual_inc_capped', 'term_clean', 'total_bc_limit_capped',\n",
        "    'total_il_high_credit_limit_capped', 'total_acc'\n",
        "]\n",
        "categorical_features = ['purpose', 'home_ownership']\n",
        "\n",
        "# 2. Preprocessing pipelines\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))    # Convert categories to numeric\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Train/test split (20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Model definition (Random Forest Classifier)\n",
        "rf_clf = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 5. Train the model\n",
        "print(\"Training the model... (This may take 1â€“2 minutes depending on dataset size)\")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Prediction and evaluation\n",
        "print(\"Generating predictions on the test set...\")\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Model Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\\n\")\n",
        "print(classification_report(\n",
        "    y_test, y_pred,\n",
        "    target_names=['Low (0â€“10k)', 'Medium (10â€“20k)', 'High (20k+)']\n",
        "))\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"\\nConfusion Matrix:\\n\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
