{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“˜ **EDA â€” Hardship & Settlement Analysis**\n",
        "\n",
        "This notebook focuses on performing Exploratory Data Analysis (EDA) on customer hardship and settlement information. The goal is to understand the structure, patterns, and behaviors associated with financial hardship events and debt settlement activities.\n",
        "\n",
        "## ðŸŽ¯ **Objectives of This Notebook**\n",
        "- Explore the dataset containing combined hardship and settlement fields  \n",
        "- Split the dataset into meaningful subsets for focused analysis  \n",
        "- Clean and standardize the data using a reusable preprocessing function  \n",
        "- Examine patterns in hardship events such as hardship types, durations, and loan impacts  \n",
        "- Investigate settlement behaviors including settlement amounts, timing, and customer characteristics  \n",
        "- Generate insights that may support risk assessment, customer segmentation, or policy strategies  \n",
        "\n",
        "By the end of this notebook, we establish a clear understanding of the hardship and settlement dynamics within the portfolio, enabling deeper modeling or reporting work.\n"
      ],
      "metadata": {
        "id": "mjtVIlYdo1XT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š **1. Loading the Hardship & Settlement Dataset**\n",
        "\n",
        "In this step, the dataset containing hardship and settlement information is loaded from the CSV file. The initial shape of the dataset and the list of available columns are inspected to understand the structure before further exploration.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Read the `hardship_settlement.csv` file into a pandas DataFrame  \n",
        "- Display the dataset shape (rows, columns)  \n",
        "- Preview the column names to understand variable availability  \n"
      ],
      "metadata": {
        "id": "P_46Tn8u3FYi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6uI1ird63S22"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hardship_settlement.csv\")\n",
        "df.shape, df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§© **2. Splitting the Dataset into Hardship and Settlement Subsets**\n",
        "\n",
        "Since the dataset includes both *hardship-related* and *settlement-related* fields, this section separates them into two dedicated DataFrames for cleaner and more focused analysis.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Define column groups related to **hardship** and **settlement**  \n",
        "- Create two new DataFrames (`hardship_df` and `settlement_df`) by selecting only the relevant columns  \n",
        "- Display the first few rows of each subset for validation  \n",
        "\n",
        "This separation allows targeted exploratory analysis for each domain.\n"
      ],
      "metadata": {
        "id": "ZDVRVHKK3S7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hardship_cols = [\n",
        "    \"id\",\n",
        "    \"hardship_flag\",\n",
        "    \"hardship_type\",\n",
        "    \"hardship_reason\",\n",
        "    \"hardship_status\",\n",
        "    \"deferral_term\",\n",
        "    \"hardship_amount\",\n",
        "    \"hardship_start_date\",\n",
        "    \"hardship_end_date\",\n",
        "    \"payment_plan_start_date\",\n",
        "    \"hardship_length\",\n",
        "    \"hardship_dpd\",\n",
        "    \"hardship_loan_status\",\n",
        "    \"orig_projected_additional_accrued_interest\",\n",
        "    \"hardship_payoff_balance_amount\",\n",
        "    \"hardship_last_payment_amount\",\n",
        "]\n",
        "\n",
        "settlement_cols = [\n",
        "    \"id\",\n",
        "    \"debt_settlement_flag\",\n",
        "    \"debt_settlement_flag_date\",\n",
        "    \"settlement_status\",\n",
        "    \"settlement_date\",\n",
        "    \"settlement_amount\",\n",
        "    \"settlement_percentage\",\n",
        "    \"settlement_term\",\n",
        "]\n",
        "\n",
        "hardship_df = df[hardship_cols].copy()\n",
        "settlement_df = df[settlement_cols].copy()\n",
        "\n",
        "hardship_df.head(), settlement_df.head()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-GhLBKnM3sLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§¼ **3. Creating a Reusable Cleaning Function**\n",
        "\n",
        "A general-purpose cleaning function is defined to standardize the preprocessing applied to both hardship and settlement datasets. This ensures consistent formatting and prepares each dataset for deeper analysis.\n",
        "\n",
        "### âœ” Cleaning Steps Applied\n",
        "- Trim all column names to remove unintended whitespace  \n",
        "- Strip whitespace in text columns and convert placeholder values (e.g., `\"None\"`, `\"nan\"`, `\"NA\"`) into proper `NaN`  \n",
        "- Convert selected columns into numeric format  \n",
        "- Convert date-like columns into `datetime`  \n",
        "- Drop columns that are entirely empty  \n",
        "- Remove duplicate rows to avoid redundant observations  \n",
        "\n",
        "This function provides a clean and uniform structure, reducing noise and improving data reliability for downstream EDA tasks.\n"
      ],
      "metadata": {
        "id": "AyKQXLgW3eED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def clean_df(dataframe, numeric_cols=None, date_cols=None):\n",
        "    df_clean = dataframe.copy()\n",
        "\n",
        "    df_clean.columns = df_clean.columns.str.strip()\n",
        "\n",
        "    obj_cols = df_clean.select_dtypes(include=\"object\").columns\n",
        "    for col in obj_cols:\n",
        "        df_clean[col] = (\n",
        "            df_clean[col]\n",
        "            .astype(str)\n",
        "            .str.strip()\n",
        "            .replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NA\": np.nan})\n",
        "        )\n",
        "\n",
        "    if numeric_cols is not None:\n",
        "        for col in numeric_cols:\n",
        "            if col in df_clean.columns:\n",
        "                df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
        "\n",
        "    if date_cols is not None:\n",
        "        for col in date_cols:\n",
        "            if col in df_clean.columns:\n",
        "                df_clean[col] = pd.to_datetime(df_clean[col], errors=\"coerce\")\n",
        "\n",
        "    df_clean = df_clean.dropna(axis=1, how=\"all\")\n",
        "\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "\n",
        "    return df_clean\n"
      ],
      "metadata": {
        "id": "sLO5Lrr24WCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§¼ **4. Cleaning Hardship and Settlement DataFrames**\n",
        "\n",
        "In this section, numeric and date columns for both the hardship and settlement datasets are specified, and the previously defined cleaning function is applied. This ensures standardized formatting, consistent data types, and removal of noise before deeper exploration.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Identify numeric and date columns relevant to **hardship** records  \n",
        "- Identify numeric and date columns relevant to **settlement** records  \n",
        "- Apply the reusable `clean_df()` function separately to:\n",
        "  - `hardship_df` â†’ producing `hardship_clean`  \n",
        "  - `settlement_df` â†’ producing `settlement_clean`  \n",
        "- Inspect the cleaned DataFrames using `.info()` to verify:\n",
        "  - Correct data types  \n",
        "  - Successful numeric/date conversions  \n",
        "  - Removal of empty or duplicate columns  \n",
        "\n",
        "This step establishes a clean and reliable foundation for all subsequent EDA tasks.\n"
      ],
      "metadata": {
        "id": "Qu7jY1aJ33IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hardship_numeric_cols = [\n",
        "    \"hardship_amount\",\n",
        "    \"hardship_length\",\n",
        "    \"hardship_dpd\",\n",
        "    \"orig_projected_additional_accrued_interest\",\n",
        "    \"hardship_payoff_balance_amount\",\n",
        "    \"hardship_last_payment_amount\",\n",
        "]\n",
        "\n",
        "hardship_date_cols = [\n",
        "    \"hardship_start_date\",\n",
        "    \"hardship_end_date\",\n",
        "    \"payment_plan_start_date\",\n",
        "]\n",
        "\n",
        "settlement_numeric_cols = [\n",
        "    \"settlement_amount\",\n",
        "    \"settlement_percentage\",\n",
        "    \"settlement_term\",\n",
        "]\n",
        "\n",
        "settlement_date_cols = [\n",
        "    \"debt_settlement_flag_date\",\n",
        "    \"settlement_date\",\n",
        "]\n",
        "\n",
        "hardship_clean = clean_df(\n",
        "    hardship_df,\n",
        "    numeric_cols=hardship_numeric_cols,\n",
        "    date_cols=hardship_date_cols,\n",
        ")\n",
        "\n",
        "settlement_clean = clean_df(\n",
        "    settlement_df,\n",
        "    numeric_cols=settlement_numeric_cols,\n",
        "    date_cols=settlement_date_cols,\n",
        ")\n",
        "\n",
        "hardship_clean.info()\n",
        "settlement_clean.info()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B5iPnl2x48ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”¤ **5. Standardizing Text Columns to Lowercase**\n",
        "\n",
        "To ensure consistent formatting across textual fields, this step converts all string-based columns in both the hardship and settlement datasets to lowercase. This prevents mismatches during grouping, filtering, or merging operations caused by inconsistent capitalization.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Identify all columns with `object` or `category` data types  \n",
        "- Convert only non-null string values to lowercase (NaN values remain unchanged)  \n",
        "- Apply the transformation to both:\n",
        "  - `hardship_clean`  \n",
        "  - `settlement_clean`  \n",
        "- Display the updated data types to confirm the transformation  \n",
        "\n",
        "This standardization step improves data quality and helps avoid case-sensitive inconsistencies in later analysis.\n"
      ],
      "metadata": {
        "id": "pA6cfPWj4XLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_text_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    text_cols = df_copy.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "\n",
        "    for col in text_cols:\n",
        "        df_copy[col] = df_copy[col].str.lower()\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "hardship_clean = lowercase_text_columns(hardship_clean)\n",
        "settlement_clean = lowercase_text_columns(settlement_clean)\n",
        "\n",
        "hardship_clean.dtypes, settlement_clean.dtypes\n"
      ],
      "metadata": {
        "id": "GWP3BvLh-Dqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¾ **6. Exporting Cleaned Hardship and Settlement Data**\n",
        "\n",
        "After completing all preprocessing and standardization steps, the cleaned datasets are exported as separate CSV files. These outputs will be used for downstream analysis or modeling tasks.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Exported files exclude the index for a clean tabular structure  \n",
        "\n",
        "This step finalizes the data preparation workflow and provides clean, ready-to-use datasets for further exploration.\n"
      ],
      "metadata": {
        "id": "IVkskNiB4zKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hardship_clean.to_csv(\"hardship_clean.csv\", index=False)\n",
        "settlement_clean.to_csv(\"settlement_clean.csv\", index=False)"
      ],
      "metadata": {
        "id": "mhHaqMrH5je9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”Ž **7. Running a Comprehensive EDA Summary Function**\n",
        "\n",
        "This section defines a reusable exploratory analysis function that provides a structured overview of any given DataFrame. The goal is to quickly understand dataset composition, data quality, and variable characteristics without writing repetitive code.\n",
        "\n",
        "### âœ” Key Features of the Function\n",
        "- Display total number of rows and columns  \n",
        "- Show data types of all variables  \n",
        "- Identify missing values with counts and percentages  \n",
        "- Report the number of unique values (top 20 columns)  \n",
        "- Separate numerical and categorical columns  \n",
        "- Provide summary statistics for numerical columns  \n",
        "- Display the top frequent categories for categorical columns  \n",
        "\n",
        "### âœ” Why This Is Useful\n",
        "- Helps validate the integrity and structure of both **hardship** and **settlement** datasets  \n",
        "- Quickly reveals data issues such as high missing rates, inconsistent categories, or extreme cardinality  \n",
        "- Enables efficient comparison across datasets during EDA  \n",
        "- Provides a standardized diagnostic tool for future notebooks  \n"
      ],
      "metadata": {
        "id": "tuf7hmfU5sT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "def basic_eda(df: pd.DataFrame, name: str = \"df\"):\n",
        "    print(f\"====== {name} â€“ General Information ======\")\n",
        "    print(f\"Number of rows: {df.shape[0]}\")\n",
        "    print(f\"Number of columns: {df.shape[1]}\")\n",
        "    print(\"\\nData types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Missing value analysis\n",
        "    print(\"\\nMissing values (count and ratio):\")\n",
        "    missing = df.isna().sum()\n",
        "    missing = missing[missing > 0].sort_values(ascending=False)\n",
        "    if missing.empty:\n",
        "        print(\"No missing values.\")\n",
        "    else:\n",
        "        missing_ratio = (missing / len(df)).round(4)\n",
        "        missing_df = pd.DataFrame({\n",
        "            \"missing_count\": missing,\n",
        "            \"missing_ratio\": missing_ratio\n",
        "        })\n",
        "        print(missing_df)\n",
        "\n",
        "    # Unique value counts\n",
        "    print(\"\\nNumber of unique values per column (top 20):\")\n",
        "    nunique = df.nunique().sort_values(ascending=False).head(20)\n",
        "    print(nunique)\n",
        "\n",
        "    # Numerical and categorical column counts\n",
        "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\", \"Int64\", \"Float64\"]).columns.tolist()\n",
        "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "    print(f\"\\nNumber of numerical columns: {len(num_cols)}\")\n",
        "    print(f\"Number of categorical columns: {len(cat_cols)}\")\n",
        "\n",
        "    # Summary statistics for numerical columns\n",
        "    if len(num_cols) > 0:\n",
        "        print(\"\\nSummary statistics for numerical columns:\")\n",
        "        print(df[num_cols].describe().T)\n",
        "\n",
        "    # Most frequent categories for categorical columns\n",
        "    if len(cat_cols) > 0:\n",
        "        print(\"\\nMost frequent values for categorical columns (top 3 categories):\")\n",
        "        for col in cat_cols[:10]:  # if many columns exist, show only the first 10\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            print(df[col].value_counts(dropna=False).head(3))\n"
      ],
      "metadata": {
        "id": "e7UZKQIr6WZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ˆ **8. Visualizing Numerical Distributions**\n",
        "\n",
        "This section introduces a helper function that visualizes the distribution of numerical features using both histograms and boxplots. These visual tools make it easier to identify outliers, detect skewness, and understand the spread of numerical variables.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Automatically detect numerical columns in the dataset  \n",
        "- Display a message if no numerical fields are present  \n",
        "- For each of the first selected numerical columns:\n",
        "  - Plot a histogram with a KDE curve to show the distribution shape  \n",
        "  - Plot a boxplot to highlight outliers and distribution spread  \n",
        "- Limit the number of plotted columns using the `max_cols` parameter to keep visual output manageable  \n",
        "\n",
        "This function enhances exploratory analysis by providing quick and consistent visual summaries of numerical features.\n"
      ],
      "metadata": {
        "id": "eHk-Gopk6eMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_numeric_distributions(df: pd.DataFrame, name: str = \"df\", max_cols: int = 6):\n",
        "    # Identify numeric columns\n",
        "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\", \"Int64\", \"Float64\"]).columns.tolist()\n",
        "    if len(num_cols) == 0:\n",
        "        print(f\"No numeric columns found in {name}.\")\n",
        "        return\n",
        "\n",
        "    print(f\"{name} â€“ Numerical column distributions (first {max_cols} columns):\")\n",
        "\n",
        "    for col in num_cols[:max_cols]:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # Histogram\n",
        "        sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
        "        axes[0].set_title(f\"{col} â€“ Histogram\")\n",
        "\n",
        "        # Boxplot\n",
        "        sns.boxplot(x=df[col], ax=axes[1])\n",
        "        axes[1].set_title(f\"{col} â€“ Boxplot\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "7D7AEL2g7F9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§© **9. Visualizing Categorical Feature Distributions**\n",
        "\n",
        "This section introduces a helper function designed to visualize the frequency distribution of categorical variables. These bar charts make it easy to identify dominant categories, rare labels, and potential inconsistencies within the dataset.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Automatically detect categorical fields (object, category, boolean)  \n",
        "- Display a message if the dataset contains no categorical variables  \n",
        "- For each of the first selected categorical columns:\n",
        "  - Compute the top 10 most frequent categories  \n",
        "  - Plot a bar chart showing category counts  \n",
        "  - Rotate labels for improved readability  \n",
        "\n",
        "This function provides a quick and interpretable overview of categorical data, supporting downstream segmentation, cleaning, or feature engineering tasks.\n"
      ],
      "metadata": {
        "id": "CJp2p73l68dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_categorical_distributions(df: pd.DataFrame, name: str = \"df\", max_cols: int = 6):\n",
        "    # Identify categorical columns\n",
        "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "    if len(cat_cols) == 0:\n",
        "        print(f\"No categorical columns found in {name}.\")\n",
        "        return\n",
        "\n",
        "    print(f\"{name} â€“ Categorical column distributions (first {max_cols} columns):\")\n",
        "\n",
        "    for col in cat_cols[:max_cols]:\n",
        "        vc = df[col].value_counts(dropna=False).head(10)  # top 10 most frequent categories\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        sns.barplot(x=vc.index.astype(str), y=vc.values)\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.title(f\"{col} â€“ Category Frequencies (Top 10)\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.xlabel(col)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "jMsIQPtf7ITU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”— **10. Correlation Analysis of Numerical Features**\n",
        "\n",
        "This section provides a helper function that computes and visualizes the correlation matrix for all numerical fields within a dataset. By examining pairwise correlations, we can detect multicollinearity, strong linear relationships, or unexpected variable interactions.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Automatically identify all numerical columns  \n",
        "- Display a message if fewer than two numerical fields are available  \n",
        "- Compute the correlation matrix using Pearson correlation  \n",
        "- Visualize the matrix using a heatmap with a diverging color palette  \n",
        "- Center the color scale at zero to highlight positive and negative correlations  \n",
        "\n",
        "This visualization supports feature selection, risk analysis, and deeper understanding of variable dependencies.\n"
      ],
      "metadata": {
        "id": "eX_lP9sc7MRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_correlation(df: pd.DataFrame, name: str = \"df\"):\n",
        "    # Identify numeric columns\n",
        "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\", \"Int64\", \"Float64\"]).columns.tolist()\n",
        "\n",
        "    # Check if correlation can be computed\n",
        "    if len(num_cols) < 2:\n",
        "        print(f\"Not enough numeric columns to compute correlation for {name}.\")\n",
        "        return\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    corr = df[num_cols].corr()\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=False, cmap=\"coolwarm\", center=0)\n",
        "    plt.title(f\"{name} â€“ Numerical Feature Correlation Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ok-y0l__7Pvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§ª **11. Running Complete EDA on Hardship and Settlement Data**\n",
        "\n",
        "In this step, the full exploratory analysis workflow is executed for both the hardship and settlement datasets. This includes structural inspection, distribution analysis, categorical exploration, and correlation evaluation.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Run the `basic_eda()` function to review:\n",
        "  - Dataset structure  \n",
        "  - Missing values  \n",
        "  - Unique value counts  \n",
        "  - Numerical and categorical summaries  \n",
        "- Visualize numerical feature distributions using `plot_numeric_distributions()`  \n",
        "- Visualize categorical feature distributions using `plot_categorical_distributions()`  \n",
        "- Analyze correlations between numerical variables with `plot_correlation()`  \n",
        "- Perform all analyses separately for:\n",
        "  - **hardship_clean**\n",
        "  - **settlement_clean**\n",
        "\n",
        "This step consolidates all previously defined EDA tools into a complete diagnostic review of both datasets."
      ],
      "metadata": {
        "id": "hqFMCtp878-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed, load cleaned datasets\n",
        "# hardship_clean = pd.read_csv(\"hardship_clean.csv\")\n",
        "# settlement_clean = pd.read_csv(\"settlement_clean.csv\")\n",
        "\n",
        "# Run full EDA summary\n",
        "basic_eda(hardship_clean, \"hardship_clean\")\n",
        "basic_eda(settlement_clean, \"settlement_clean\")\n",
        "\n",
        "# Numerical distributions\n",
        "plot_numeric_distributions(hardship_clean, \"hardship_clean\", max_cols=6)\n",
        "plot_numeric_distributions(settlement_clean, \"settlement_clean\", max_cols=6)\n",
        "\n",
        "# Categorical distributions\n",
        "plot_categorical_distributions(hardship_clean, \"hardship_clean\", max_cols=6)\n",
        "plot_categorical_distributions(settlement_clean, \"settlement_clean\", max_cols=6)\n",
        "\n",
        "# Correlation analysis\n",
        "plot_correlation(hardship_clean, \"hardship_clean\")\n",
        "plot_correlation(settlement_clean, \"settlement_clean\")"
      ],
      "metadata": {
        "id": "y6CXf99Y7Uds"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}