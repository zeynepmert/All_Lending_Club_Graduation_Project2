{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š **Exploratory Data Analysis (EDA)**\n",
        "\n",
        "This notebook is designed to explore the cleaned *delinquency risk* dataset and gain a deeper understanding of its structure and characteristics.\n",
        "\n",
        "## ðŸŽ¯ **Objectives of EDA**\n",
        "\n",
        "- Understand the overall structure of the dataset (rows, columns, data types)  \n",
        "- Identify missing values, inconsistencies, and potential data quality issues  \n",
        "- Analyze the distributions of numerical and categorical features  \n",
        "- Detect patterns, anomalies, and potential outliers  \n",
        "- Generate insights that can support future modeling or feature engineering steps  \n",
        "\n",
        "In this stage, **no modifications are made to the data** â€” the dataset is only analyzed."
      ],
      "metadata": {
        "id": "OdLVDiwFo8y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š **1. Loading Required Libraries**\n",
        "\n",
        "In the following cell, essential libraries such as pandas are imported, and the dataset is loaded for initial exploration.  \n",
        "Basic structural and statistical summaries are also displayed."
      ],
      "metadata": {
        "id": "LYnZUeTXuSHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nM0l3KY9w2wO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df_clean = pd.read_csv(\"delinquency_risk_cleaned.csv\")\n",
        "\n",
        "print(\"SHAPE:\", df_clean.shape)\n",
        "\n",
        "print(\"\\nINFO\\n\")\n",
        "df_clean.info()\n",
        "\n",
        "print(\"\\nDESCRIBE NUMERIC\\n\")\n",
        "display(df_clean.describe())\n",
        "\n",
        "print(\"\\nDESCRIBE CATEGORICAL\\n\")\n",
        "display(df_clean.describe(include='object'))\n",
        "\n",
        "print(\"\\nHEAD\\n\")\n",
        "display(df_clean.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”¢ **2. Identifying Numerical and Categorical Columns**\n",
        "\n",
        "In this section, we separate numerical and categorical features to structure the exploratory analysis. This helps ensure that each variable type is analyzed using appropriate statistical and visualization techniques.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Create a working copy of the cleaned dataset  \n",
        "- Identify all numerical columns using `select_dtypes`  \n",
        "- Exclude technical identifiers such as **id** from analysis  \n",
        "- Identify categorical columns  \n",
        "- Generate a descriptive summary table for numerical features  \n",
        "- Calculate the **zero ratio**, showing the proportion of zero values in each numerical column\n",
        "\n",
        "These steps provide a clear overview of the dataset's feature types and allow deeper analysis in later sections.\n"
      ],
      "metadata": {
        "id": "xeB4No3JpIEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_eda = df_clean.copy()\n",
        "\n",
        "num_cols = df_eda.select_dtypes(include=[np.number]).columns\n",
        "num_cols = [c for c in num_cols if c.lower() != \"id\"]\n",
        "\n",
        "cat_cols = df_eda.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "\n",
        "print(\"SayÄ±sal kolonlar:\", num_cols)\n",
        "print(\"Kategorik kolonlar:\", list(cat_cols))\n",
        "\n",
        "num_summary = df_eda[num_cols].describe().T\n",
        "num_summary[\"zero_ratio\"] = (df_eda[num_cols] == 0).mean()\n",
        "num_summary[[\"count\",\"mean\",\"std\",\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"zero_ratio\"]]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fzPnjKjp3x6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ˆ **3. Visualizing Numerical Feature Distributions**\n",
        "\n",
        "To better understand the behavior of numerical variables, this section visualizes each numerical column using both a histogram (with KDE) and a boxplot.  \n",
        "These visualizations help identify:\n",
        "\n",
        "- The distribution shape (normal, skewed, multimodal)  \n",
        "- Presence of outliers  \n",
        "- Potential data quality issues  \n",
        "- Range, spread, and central tendencies  \n",
        "\n",
        "### âœ” How the function works\n",
        "- A custom function `plot_numeric_distribution()` is defined  \n",
        "- For each numerical column, the function draws:  \n",
        "  - **Histogram + KDE curve**  \n",
        "  - **Boxplot** to highlight outliers  \n",
        "- The function is applied to all numerical features in `num_cols`"
      ],
      "metadata": {
        "id": "1jaOMkeGpmGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_numeric_distribution(df, col):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
        "    axes[0].set_title(f\"{col} - Histogram/KDE\")\n",
        "\n",
        "    sns.boxplot(x=df[col], ax=axes[1])\n",
        "    axes[1].set_title(f\"{col} - Boxplot\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for col in num_cols:\n",
        "    plot_numeric_distribution(df_eda, col)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9DKBL0KC6AUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§© **4. Exploring Categorical Feature Distributions**\n",
        "\n",
        "This section focuses on examining the distribution of categorical variables. Understanding the frequency of category values helps identify dominant groups, rare categories, and potential inconsistencies.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- For each categorical column, the top 10 most frequent categories are displayed  \n",
        "- A bar chart is generated to visualize the frequency distribution  \n",
        "- Category labels are rotated for readability  \n",
        "- This process helps detect data imbalance, uncommon labels, or potential issues before modeling\n",
        "\n",
        "### Code Logic (plain text)\n",
        "for each column in cat_cols:  \n",
        "â€¢ print its top 10 value counts  \n",
        "â€¢ plot a bar chart of the top 10 categories  \n",
        "â€¢ display the plot with appropriate title and formatting\n"
      ],
      "metadata": {
        "id": "ktLO1-oVqL8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in cat_cols:\n",
        "    print(f\"\\n{col} value_counts:\")\n",
        "    print(df_eda[col].value_counts().head(10))\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    df_eda[col].value_counts().head(10).plot(kind=\"bar\")\n",
        "    plt.title(f\"Top 10 categories - {col}\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mF0l9kU-6bC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”— **5. Correlation Analysis of Numerical Features**\n",
        "\n",
        "In this section, a correlation matrix is computed to examine relationships between numerical variables. Correlation analysis helps identify potential multicollinearity, strong linear relationships, or variables that may contribute similarly to predictive models.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Select all numerical features except the identifier column  \n",
        "- Compute the pairwise correlation matrix  \n",
        "- Visualize the matrix using a heatmap with a diverging color palette  \n",
        "- Highlight positive and negative correlations centered around zero  \n",
        "\n",
        "### Code Logic (plain text)\n",
        "â€¢ extract numerical columns  \n",
        "â€¢ remove the â€œidâ€ field from analysis  \n",
        "â€¢ compute correlations using df_eda[num_cols].corr()  \n",
        "â€¢ display a heatmap (coolwarm, center=0) for clearer interpretation\n"
      ],
      "metadata": {
        "id": "xb_vuUscqh7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#korelasyon matrisi\n",
        "num_cols = df_eda.select_dtypes(include=[np.number]).columns\n",
        "num_cols = [c for c in num_cols if c.lower() != \"id\"]\n",
        "\n",
        "corr = df_eda[num_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation Matrix (Numeric Features)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6iDB9wZr93pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“‰ **6. Detailed Distribution Plots for Numerical Features**\n",
        "\n",
        "This section provides a more detailed visualization of each numerical variable by displaying both a histogram (with KDE) and a boxplot side by side. These visualizations help deepen the understanding of numerical feature behavior and potential anomalies.\n",
        "\n",
        "### âœ” Insights Gained\n",
        "- Histogram + KDE reveals the overall distribution shape  \n",
        "- Boxplot highlights outliers and the spread of values  \n",
        "- Combined, these plots offer a clear view of skewness, variance, and extreme values  \n",
        "- Supports decisions for potential transformations or normalization steps  \n",
        "\n",
        "### Code Logic (plain text)\n",
        "for each numerical column:  \n",
        "â€¢ create a two-panel figure  \n",
        "â€¢ left panel â†’ histogram + KDE curve  \n",
        "â€¢ right panel â†’ boxplot for outlier detection  \n",
        "â€¢ adjust layout and display the plots  \n"
      ],
      "metadata": {
        "id": "4Pww7t5MrAcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in num_cols:\n",
        "    plt.figure(figsize=(10,4))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.histplot(df_eda[col].dropna(), kde=True)\n",
        "    plt.title(f\"{col} - Histogram/KDE\")\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    sns.boxplot(x=df_eda[col])\n",
        "    plt.title(f\"{col} - Boxplot\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kEFU3o9L5aeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš¦ **7. Creating Risk Segments Based on Time Since Last Delinquency**\n",
        "\n",
        "In this section, customers are segmented according to the number of months since their last delinquency event. This transformation helps classify customers into meaningful risk groups and supports downstream modeling or reporting.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Filter out customers whose delinquency value is labeled as \"Never\"  \n",
        "- Convert the delinquency field into a numeric format  \n",
        "- Define risk segments using `pd.cut` based on time intervals  \n",
        "- Count the number of customers in each risk group  \n",
        "- Visualize the resulting distribution using a bar chart  \n",
        "\n",
        "### âœ” Insights Gained\n",
        "- Customers with recent delinquency (0â€“6 months) represent the **highest risk**  \n",
        "- Those with 6â€“24 months since delinquency fall into a **medium-risk** category  \n",
        "- Customers with 24+ months show signs of **improvement or recovery**  \n",
        "- This segmentation allows institutions to tailor risk strategies and customer approaches  \n"
      ],
      "metadata": {
        "id": "5TwO-0iisypc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_delinq = df_clean[df_clean[\"mths_since_last_delinq\"] != \"Never\"].copy()\n",
        "\n",
        "# 2) Convert to numeric\n",
        "df_delinq[\"mths_since_last_delinq_num\"] = pd.to_numeric(\n",
        "    df_delinq[\"mths_since_last_delinq\"],\n",
        "    errors=\"coerce\"\n",
        ")\n",
        "\n",
        "# 3) Define risk segments using pd.cut\n",
        "# 0â€“6  -> High Risk\n",
        "# 6â€“24 -> Medium Risk\n",
        "# 24+  -> Recovery Signal\n",
        "\n",
        "bins = [0, 6, 24, np.inf]\n",
        "labels = [\"High Risk 0â€“6 Months\", \"Medium Risk 6â€“24 Months\", \"Recovery Signal 24+ Months\"]\n",
        "\n",
        "df_delinq[\"risk_segment\"] = pd.cut(\n",
        "    df_delinq[\"mths_since_last_delinq_num\"],\n",
        "    bins=bins,\n",
        "    labels=labels,\n",
        "    right=False  # 0-6, 6-24, 24+\n",
        ")\n",
        "\n",
        "# 4) Count customers by risk segments\n",
        "risk_counts = df_delinq[\"risk_segment\"].value_counts().reindex(labels)\n",
        "\n",
        "print(risk_counts)\n",
        "\n",
        "# 5) Plot bar chart\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=risk_counts.index, y=risk_counts.values)\n",
        "plt.title(\"Risk Segments Based on Months Since Last Delinquency\")\n",
        "plt.xlabel(\"Risk Segment\")\n",
        "plt.ylabel(\"Customer Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-RIy4xC97ie7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§­ **8. Constructing a Comprehensive Behavioral Risk Profile**\n",
        "\n",
        "In this section, a consolidated **risk_profile** variable is created by combining multiple behavioral indicators such as recent delinquency, charge-off events, credit card delays, and severe past-due history.  \n",
        "The objective is to classify customers into prioritized risk tiers based on their credit behavior.\n",
        "\n",
        "### âœ” Steps Performed\n",
        "- Prepare a numeric version of the `mths_since_recent_revol_delinq` column (convert â€œNeverâ€ â†’ NaN)  \n",
        "- Define behavioral risk flags:\n",
        "  - **Mild delay** â€“ presence of 30+ day delinquency  \n",
        "  - **Credit cardâ€“driven delinquency** â€“ recent revolving delinquency (â‰¤ 3 months)  \n",
        "  - **Charge-off / collections history** â€“ severe financial distress indicators  \n",
        "  - **Severe past-due accounts** â€“ 120+ day delinquency at any time  \n",
        "- Combine these indicators into a single prioritized risk profile using `np.select`  \n",
        "- Compute counts and percentages for each risk segment  \n",
        "- Visualize the distribution with a bar chart  \n",
        "\n",
        "### âœ” Insights Gained\n",
        "- The profile hierarchy emphasizes *severity first*, ensuring customers with charge-offs or severe delinquencies are captured at the top of the risk spectrum  \n",
        "- Recent revolving delinquency helps detect short-term credit card stress  \n",
        "- The final profile provides a clean, interpretable segmentation for reporting or modeling  \n"
      ],
      "metadata": {
        "id": "EbriqLP-tsqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) DataFrame to work with\n",
        "df_seg = df_clean.copy()\n",
        "\n",
        "# 1) Prepare numeric column for mths_since_recent_revol_delinq (Never -> NaN)\n",
        "if \"mths_since_recent_revol_delinq_num\" in df_seg.columns:\n",
        "    num_col = \"mths_since_recent_revol_delinq_num\"\n",
        "else:\n",
        "    df_seg[\"mths_since_recent_revol_delinq_num\"] = pd.to_numeric(\n",
        "        df_seg[\"mths_since_recent_revol_delinq\"].replace(\"Never\", np.nan),\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "    num_col = \"mths_since_recent_revol_delinq_num\"\n",
        "\n",
        "# 2) Define flags according to your rules\n",
        "\n",
        "# Mild delay history: num_tl_30dpd > 0\n",
        "mild_delay_flag = df_seg[\"num_tl_30dpd\"] > 0\n",
        "\n",
        "# Credit cardâ€“driven delinquency: mths_since_recent_revol_delinq close to 0\n",
        "# Here, \"close to 0\" is defined as last 3 months (<= 3). You may adjust to 6 or 12 months if needed.\n",
        "cc_recent_flag = df_seg[num_col].notna() & (df_seg[num_col] <= 3)\n",
        "\n",
        "# Charge-off / collections history: chargeoff_within_12_mths > 0 or collections_12_mths_ex_med > 0\n",
        "chargeoff_flag = (df_seg[\"chargeoff_within_12_mths\"] > 0) | \\\n",
        "                 (df_seg[\"collections_12_mths_ex_med\"] > 0)\n",
        "\n",
        "# Severe delinquency history: num_accts_ever_120_pd > 0\n",
        "heavy_delay_flag = df_seg[\"num_accts_ever_120_pd\"] > 0\n",
        "\n",
        "# 3) Create a single risk_profile column based on priority order\n",
        "# Priority: Charge-off > Severe Delay > Credit Card Delay > Mild Delay > Clean Profile\n",
        "\n",
        "conditions = [\n",
        "    chargeoff_flag,\n",
        "    heavy_delay_flag,\n",
        "    cc_recent_flag,\n",
        "    mild_delay_flag\n",
        "]\n",
        "\n",
        "choices = [\n",
        "    \"Charge-off / Collections History\",\n",
        "    \"Severe Delinquency History\",\n",
        "    \"Credit Cardâ€“Driven Delinquency\",\n",
        "    \"Mild Delinquency History\"\n",
        "]\n",
        "\n",
        "df_seg[\"risk_profile\"] = np.select(\n",
        "    conditions,\n",
        "    choices,\n",
        "    default=\"Clean Profile\"\n",
        ")\n",
        "\n",
        "# 4) Segment distribution (count and percentage)\n",
        "order = [\n",
        "    \"Clean Profile\",\n",
        "    \"Mild Delinquency History\",\n",
        "    \"Credit Cardâ€“Driven Delinquency\",\n",
        "    \"Severe Delinquency History\",\n",
        "    \"Charge-off / Collections History\"\n",
        "]\n",
        "\n",
        "profile_counts = df_seg[\"risk_profile\"].value_counts().reindex(order)\n",
        "profile_pct = profile_counts / len(df_seg)\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"count\": profile_counts,\n",
        "    \"ratio\": (profile_pct * 100).round(2)\n",
        "})\n",
        "print(summary)\n",
        "\n",
        "# 5) Plot bar chart\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=profile_counts.index, y=profile_counts.values)\n",
        "plt.title(\"Customer Risk Profile Distribution\")\n",
        "plt.xlabel(\"Risk Profile\")\n",
        "plt.ylabel(\"Customer Count\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eTVPibjg-bIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}